{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proj: Bandit-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \n",
    "$$\\theta_1 < \\theta_2 < \\theta_3$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 设置随机数种子（以保证可重复性）\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_truncated_normal_data(mean, variance, size):\n",
    "    std_dev = np.sqrt(variance)\n",
    "    \n",
    "    # 生成符合正态分布的数据\n",
    "    data = np.random.normal(mean, std_dev, size)\n",
    "    \n",
    "    # 将数据截断到0-1的范围\n",
    "    data = np.clip(data, 0, 1)\n",
    "    \n",
    "    # 如果需要的数据量较少，可以多生成一些数据供截断后选择\n",
    "    while len(data) < size:\n",
    "        extra_data = np.random.normal(mean, std_dev, size * 2)\n",
    "        extra_data = np.clip(extra_data, 0, 1)\n",
    "        data = np.append(data, extra_data)\n",
    "        data = data[:size]  # 只保留先前需要的数量\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 指定均值和方差\n",
    "mean = 0.5\n",
    "variance = 0.1\n",
    "\n",
    "# 生成 3 个 0-1 之间的数据\n",
    "data = generate_truncated_normal_data(mean, variance, 3)\n",
    "\n",
    "print(\"Generated data:\", data)\n",
    "print(\"Mean of generated data:\", np.mean(data))\n",
    "print(\"Variance of generated data:\", np.var(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ThetaSequence:\n",
    "    def __init__(self, seq: np.ndarray, n_elements: int):\n",
    "        self.seq = seq\n",
    "        self.n_elem = n_elements\n",
    "        self.mean = np.mean(seq)\n",
    "        self.var = np.var(seq)\n",
    "        self.max_theta = np.max(seq)\n",
    "\n",
    "def generate_custom_theta_sequence_set(n_elem, n_sets):\n",
    "    theta_seq_set = []\n",
    "    max_std_dev = np.sqrt(1/6)\n",
    "    \n",
    "    # 生成均值和标准差的线性空间\n",
    "    mean_values = np.linspace(0, 1, int(np.sqrt(n_sets)))  # 创建平均值范围\n",
    "    std_dev_values = np.linspace(0, max_std_dev, int(np.sqrt(n_sets)))  # 创建标准差范围\n",
    "\n",
    "    for mean in mean_values:\n",
    "        for std_dev in std_dev_values:\n",
    "            seq = np.random.normal(loc=mean, scale=std_dev, size=n_elem)\n",
    "            seq = np.clip(seq, 0, 1)  # 把序列的值都限制在 0 和 1 之间\n",
    "            theta_seq = ThetaSequence(seq, n_elem)\n",
    "            theta_seq_set.append(theta_seq)\n",
    "    \n",
    "    return np.array(theta_seq_set)\n",
    "\n",
    "# 示例使用：\n",
    "n_elem = 100\n",
    "n_sets = 100\n",
    "theta_seq_set = generate_custom_theta_sequence_set(n_elem, n_sets)\n",
    "\n",
    "# 检查生成的 theta 序列集合\n",
    "for theta_seq in theta_seq_set:\n",
    "    print(f\"Mean: {theta_seq.mean:.2f}, Std Dev: {np.sqrt(theta_seq.var):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ThetaSequence:\n",
    "    def __init__(self, seq: np.ndarray, n_elements: int):\n",
    "        self.seq = seq\n",
    "        self.n_elem = n_elements\n",
    "        self.mean = np.mean(seq)\n",
    "        self.var = np.var(seq)\n",
    "        self.max_theta = np.max(seq)\n",
    "\n",
    "def generate_theta_sequence_set(n_elem, n_sets):\n",
    "    theta_seq_set = []\n",
    "    for _ in range(n_sets):\n",
    "        theta_seq = ThetaSequence(np.random.uniform(0, 1, size=n_elem), n_elem)\n",
    "        theta_seq_set.append(theta_seq)\n",
    "    return np.array(theta_seq_set)\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "n_elem = 3\n",
    "n_sets = 50\n",
    "\n",
    "theta_seq_set = generate_theta_sequence_set(n_elem, n_sets)\n",
    "\n",
    "# 提取 ThetaSequence 对象的 mean 属性作为数据序列\n",
    "mean_values = [theta_seq.mean for theta_seq in theta_seq_set]\n",
    "plt.hist(mean_values, bins=30, density=True, alpha=0.7, color='blue')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mean Values')\n",
    "plt.show()\n",
    "\n",
    "# 提取 ThetaSequence 对象的 variance 属性作为数据序列\n",
    "variance_values = [theta_seq.var for theta_seq in theta_seq_set]\n",
    "plt.hist(variance_values, bins=30, density=True, alpha=0.7, color='blue')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Variance Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ThetaSequence:\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "def generate_theta_sequence_set(n_elem, n_sets):\n",
    "    theta_seq_set = [ThetaSequence(np.random.uniform(0, 1, size=n_elem)) for _ in range(n_sets)]\n",
    "    return theta_seq_set\n",
    "\n",
    "def simulate_ucb(theta, c, num_pulls_per_experiment):\n",
    "    num_arms = len(theta)\n",
    "    total_rewards = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    \n",
    "    for _ in range(num_pulls_per_experiment):\n",
    "        if np.any(counts == 0):\n",
    "            arm = np.random.randint(num_arms)\n",
    "        else:\n",
    "            ucb_values = (total_rewards / np.maximum(counts, 1)) + np.sqrt((2 * np.log(np.sum(counts)) * c) / np.maximum(counts, 1))\n",
    "            arm = np.argmax(ucb_values)\n",
    "        counts[arm] += 1\n",
    "        reward = np.random.binomial(1, theta[arm])\n",
    "        total_rewards[arm] += reward\n",
    "\n",
    "    return total_rewards.sum()\n",
    "\n",
    "def simulate_epsilon_greedy(theta, epsilon, num_pulls_per_experiment):\n",
    "    num_arms = len(theta)\n",
    "    total_rewards = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    \n",
    "    for _ in range(num_pulls_per_experiment):\n",
    "        arm = np.random.randint(num_arms) if np.random.rand() < epsilon else np.argmax(theta)\n",
    "        counts[arm] += 1\n",
    "        reward = np.random.binomial(1, theta[arm])\n",
    "        total_rewards[arm] += reward\n",
    "\n",
    "    return total_rewards.sum()\n",
    "\n",
    "def oracle_value(theta):\n",
    "    return np.max(theta) * num_pulls_per_experiment\n",
    "\n",
    "# 设置参数\n",
    "n_elem = 3\n",
    "n_sets = 50\n",
    "ucb_cs = [1, 5, 10]\n",
    "epsilons = [0.1, 0.5, 0.9]\n",
    "num_pulls_per_experiment = 5000\n",
    "\n",
    "# 生成ThetaSequence集合\n",
    "theta_seq_set = generate_theta_sequence_set(n_elem, n_sets)\n",
    "\n",
    "# 初始化结果存储\n",
    "ucb_results = {c: [] for c in ucb_cs}\n",
    "epsilon_greedy_results = {epsilon: [] for epsilon in epsilons}\n",
    "best_params_each_experiment = []\n",
    "losses_per_experiment = []\n",
    "\n",
    "# 模拟实验并记录排名\n",
    "for idx, theta_seq in enumerate(theta_seq_set):\n",
    "    print(f\"Experiment {idx + 1}:\")\n",
    "    theta = theta_seq.seq\n",
    "    theta_oracle = oracle_value(theta)\n",
    "    all_rewards = []\n",
    "    labels = []\n",
    "    \n",
    "    # 模拟UCB策略\n",
    "    for c in ucb_cs:\n",
    "        reward = simulate_ucb(theta, c, num_pulls_per_experiment)\n",
    "        all_rewards.append(reward)\n",
    "        labels.append(f'UCB c={c}')\n",
    "        print(f\"UCB c={c}: Reward {reward:.2f}, Oracle Value {theta_oracle:.2f}\")\n",
    "    \n",
    "    # 模拟epsilon-greedy策略\n",
    "    for epsilon in epsilons:\n",
    "        reward = simulate_epsilon_greedy(theta, epsilon, num_pulls_per_experiment)\n",
    "        all_rewards.append(reward)\n",
    "        labels.append(f'Epsilon-Greedy ε={epsilon}')\n",
    "        print(f\"Epsilon-Greedy ε={epsilon}: Reward {reward:.2f}, Oracle Value {theta_oracle:.2f}\")\n",
    "    \n",
    "    # 计算损失和最接近Oracle的策略和参数\n",
    "    losses = [abs(reward - theta_oracle) for reward in all_rewards]\n",
    "    losses_per_experiment.append(losses)\n",
    "    min_loss_idx = np.argmin(losses)\n",
    "    best_param = labels[min_loss_idx]\n",
    "    print(f\"Best strategy and parameter: {best_param} with loss {losses[min_loss_idx]:.2f}\")\n",
    "    best_params_each_experiment.append(best_param)\n",
    "    \n",
    "    # 计算排名\n",
    "    ranks = np.argsort(-np.array(all_rewards))\n",
    "    rank_results = dict(zip(labels, ranks + 1))\n",
    "    \n",
    "    # 打印排名结果\n",
    "    for label, rank in rank_results.items():\n",
    "        print(f\"{label}: Rank {rank}\")\n",
    "    \n",
    "    # 按排名存储结果\n",
    "    for c in ucb_cs:\n",
    "        ucb_results[c].append(all_rewards[labels.index(f'UCB c={c}')])\n",
    "    for epsilon in epsilons:\n",
    "        epsilon_greedy_results[epsilon].append(all_rewards[labels.index(f'Epsilon-Greedy ε={epsilon}')])\n",
    "\n",
    "# 绘制结果\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制UCB策略的结果\n",
    "for c, rewards in ucb_results.items():\n",
    "    plt.plot(rewards, label=f'UCB c={c}', marker='o')\n",
    "\n",
    "# 绘制epsilon-greedy策略的结果\n",
    "for epsilon, rewards in epsilon_greedy_results.items():\n",
    "    plt.plot(rewards, label=f'Epsilon-Greedy ε={epsilon}', marker='x')\n",
    "\n",
    "plt.title('Comparison of UCB and Epsilon-Greedy Algorithms')\n",
    "plt.xlabel('Theta Sequence Index')\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.legend(title='Algorithm and Parameters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 绘制损失折线图\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 绘制UCB策略的损失\n",
    "for idx, c in enumerate(ucb_cs):\n",
    "    ucb_losses = [losses[idx] for losses in losses_per_experiment]\n",
    "    plt.plot(ucb_losses, label=f'UCB c={c}', marker='o')\n",
    "\n",
    "# 绘制epsilon-greedy策略的损失\n",
    "for idx, epsilon in enumerate(epsilons):\n",
    "    epsilon_losses = [losses[len(ucb_cs) + idx] for losses in losses_per_experiment]\n",
    "    plt.plot(epsilon_losses, label=f'Epsilon-Greedy ε={epsilon}', marker='x')\n",
    "\n",
    "plt.title('Loss Comparison of UCB and Epsilon-Greedy Algorithms')\n",
    "plt.xlabel('Theta Sequence Index')\n",
    "plt.ylabel('Loss (|Reward - Oracle Value|)')\n",
    "plt.legend(title='Algorithm and Parameters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 打印每次实验中表现最好的策略和参数\n",
    "print(\"Best parameters and strategies for each experiment:\")\n",
    "for i, best_param in enumerate(best_params_each_experiment):\n",
    "    print(f\"Experiment {i + 1}: {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThetaSequence:\n",
    "    def __init__(self, seq: np.ndarray, n_elements: int):\n",
    "        self.seq = seq\n",
    "        self.n_elem = n_elements\n",
    "        self.mean = np.mean(seq)\n",
    "        self.var = np.var(seq)\n",
    "\n",
    "## 根据theta生成beta分布先验\n",
    "def beta_prio_generate(theta_seq_set: np.ndarray, min_n_trails: int):\n",
    "  para_set_min = []\n",
    "  para_set_10_min = []\n",
    "  para_set_100_min = []\n",
    "  for theta_seq in theta_seq_set:\n",
    "    for k in range(3):\n",
    "      para = []\n",
    "      for theta in theta_seq.seq:\n",
    "        res = np.random.binomial(1, theta, (10 ** k)*min_n_trails)\n",
    "        para.append([np.sum(res), (10 ** k)*min_n_trails - np.sum(res)])\n",
    "      if k==0:\n",
    "        para_set_min.append(para)\n",
    "      elif k==1:\n",
    "        para_set_10_min.append(para)\n",
    "      else:\n",
    "        para_set_100_min.append(para)\n",
    "\n",
    "  return np.array(para_set_min), np.array(para_set_10_min), np.array(para_set_100_min)\n",
    "  \n",
    "## 性能优化：所有数组先申请好成1，最后再增加\n",
    "para_exp_0, para_exp_1, para_exp_2 = beta_prio_generate(theta_seq_set, min_n_trails=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 参数修正后的TS\n",
    "\n",
    "## 问题是没有得到最终的策略\n",
    "def Thompson_Sampling_once(n_arms, n_trials, beta_prio):\n",
    "  ## 手动都进行+1\n",
    "  for i in range(n_arms):\n",
    "    beta_prio[i][0] += 1\n",
    "    beta_prio[i][1] += 1\n",
    "  sampled_theta = np.zeros(n_arms)\n",
    "  reward = np.zeros(n_arms, dtype=int)\n",
    "  for _ in range(n_trials):\n",
    "    for i in range(n_arms):\n",
    "      sampled_theta[i] = np.random.beta(beta_prio[i][0],beta_prio[i][1])\n",
    "    max_arm = np.argmax(sampled_theta)\n",
    "    cor_reward = np.random.binomial(1, sampled_theta[max_arm])\n",
    "    beta_prio[max_arm][0] += cor_reward\n",
    "    beta_prio[max_arm][1] +=  (1 - cor_reward) \n",
    "    reward[max_arm] += cor_reward\n",
    "\n",
    "  for i in range(n_arms):\n",
    "    ## final update\n",
    "    sampled_theta[i] = np.random.beta(beta_prio[i][0],beta_prio[i][1])  \n",
    "  # print(np.sum(reward))\n",
    "  return np.sum(reward)       ##, sampled_theta   这里就不输出了\n",
    "\n",
    "final_reward_para_exp_0 = []\n",
    "final_reward_para_exp_1 = []\n",
    "final_reward_para_exp_2 = []\n",
    "\n",
    "\n",
    "print(\"I'm calculating, plz wait.\")\n",
    "for i in range(len(theta_seq_set)):\n",
    "  ## 对第i组theta做500次实验，取平均\n",
    "  reward_0, reward_1, reward_2 = [], [], []\n",
    "\n",
    "  ## 5次实验，5000 time slots，否则真有点久\n",
    "  for j in range(5):\n",
    "    reward_0.append(Thompson_Sampling_once(3, 5000, para_exp_0[i]))\n",
    "    reward_1.append(Thompson_Sampling_once(3, 5000, para_exp_1[i]))\n",
    "    reward_2.append(Thompson_Sampling_once(3, 5000, para_exp_2[i]))\n",
    "  final_reward_para_exp_0.append(np.mean(np.array(reward_0)))\n",
    "  final_reward_para_exp_1.append(np.mean(np.array(reward_1)))\n",
    "  final_reward_para_exp_2.append(np.mean(np.array(reward_2)))\n",
    "\n",
    "\n",
    "# for i in range(len(theta_seq_set)):\n",
    "#   print(str(theta_seq_set[i].max_theta * 5000)+\" \"+str(final_reward_para_exp_0[i])+\" \"+str(final_reward_para_exp_1[i])+\" \"+str(final_reward_para_exp_2[i]))\n",
    "#   print(\"delta:\" + str(final_reward_para_exp_0[i]-theta_seq_set[i].max_theta * 5000) + str(final_reward_para_exp_0[i]-theta_seq_set[i].max_theta * 5000) + str(final_reward_para_exp_0[i]-theta_seq_set[i].max_theta * 5000))\n",
    "\n",
    "# print(final_reward_para_exp_0)\n",
    "# print(final_reward_para_exp_1)\n",
    "# print(final_reward_para_exp_2)\n",
    "arm_static=[]\n",
    "for i in range(len(theta_seq_set)):\n",
    "    # 获取格式化的原数值字符串\n",
    "    values_str = \"{:.1f} {:.1f} {:.1f} {:.1f}\".format(\n",
    "        theta_seq_set[i].max_theta * 5000,\n",
    "        final_reward_para_exp_0[i],\n",
    "        final_reward_para_exp_1[i],\n",
    "        final_reward_para_exp_2[i]\n",
    "    )\n",
    "    \n",
    "    # 计算各delta值\n",
    "    delta_0 = final_reward_para_exp_0[i] - theta_seq_set[i].max_theta * 5000\n",
    "    delta_1 = final_reward_para_exp_1[i] - theta_seq_set[i].max_theta * 5000\n",
    "    delta_2 = final_reward_para_exp_2[i] - theta_seq_set[i].max_theta * 5000\n",
    "    \n",
    "    # 获取格式化的delta字符串\n",
    "    delta_str = \"delta: {:.1f} {:.1f} {:.1f}\".format(delta_0, delta_1, delta_2)\n",
    "    delta = [abs(delta_0),abs(delta_1),abs(delta_2)]\n",
    "    # 打印原数值和delta，保证delta字符串右对齐\n",
    "    print(values_str)\n",
    "    print(delta_str.rjust(len(values_str)))\n",
    "    print(\"Theta-Var: {:.4f}   best-arm: {:1}\".format(theta_seq_set[i].var, 1+np.argmin([delta])))\n",
    "    arm_static.append(1+np.argmin([delta]))\n",
    "    print()\n",
    "    ## 统计个数？\n",
    "\n",
    "# 统计1，2，3的个数\n",
    "count_1 = arm_static.count(1)\n",
    "count_2 = arm_static.count(2)\n",
    "count_3 = arm_static.count(3)\n",
    "\n",
    "# 打印统计结果\n",
    "print(f\"Count of 1: {count_1}\")\n",
    "print(f\"Count of 2: {count_2}\")\n",
    "print(f\"Count of 3: {count_3}\")\n",
    "\n",
    "# 准备绘图数据\n",
    "labels = ['10', '100', '1000']\n",
    "counts = [count_1, count_2, count_3]\n",
    "\n",
    "# 绘制条形图\n",
    "plt.bar(labels, counts, color=['blue', 'green', 'red'])\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Count')\n",
    "plt.title('xxxxxxxxx')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.4, 0.5, 0.9]    4500\n",
    "0.4     4,6       40,60      400,600\n",
    "0.5     5,5       50,50      500,500\n",
    "0.9     9,1       90,10      900,100          \n",
    "        4300      4400        4450\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "  print(theta_seq_set[i].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "## 可使用随机数生成theta\n",
    "actual_theta = np.array([0.6,0.3,0.1])\n",
    "\n",
    "\n",
    "def Thompson_Sampling(n_arms, n_trials, alpha, beta):\n",
    "  theta = np.zeros(n_arms, dtype=float)\n",
    "  reward = np.zeros(n_arms, dtype=int)\n",
    "  for _ in range(n_trials):\n",
    "\n",
    "    for i in range(n_arms):\n",
    "    ## update\n",
    "      theta[i] = np.random.beta(alpha[i], beta[i])\n",
    "    \n",
    "    ## select\n",
    "    max_arm = np.argmax(theta)\n",
    "    cor_reward = np.random.binomial(1, theta[max_arm])\n",
    "    alpha[max_arm] += cor_reward\n",
    "    beta[max_arm] +=  (1 - cor_reward)\n",
    "  \n",
    "    reward[max_arm] += cor_reward\n",
    "\n",
    "  for i in range(n_arms):\n",
    "    ## final update\n",
    "    theta[i] = np.random.beta(alpha[i], beta[i])  \n",
    "  return np.sum(reward), theta\n",
    "\n",
    "\n",
    "def TS_simulation_mean(n_arms, n_trials, alpha_set, beta_set, set_index, n_experiment):\n",
    "  simu_reward_set = []\n",
    "  esti_theta_set = []\n",
    "\n",
    "  for i in range(n_experiment):\n",
    "    simu_reward, esti_theta = Thompson_Sampling(n_arms, n_trials, alpha_set[set_index], beta_set[set_index])\n",
    "    simu_reward_set.append(simu_reward)\n",
    "    esti_theta_set.append(esti_theta)\n",
    "\n",
    "  return np.mean(np.array(simu_reward_set)), esti_theta_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS (Thompson Sampling) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "## 可使用随机数生成theta\n",
    "actual_theta = np.array([0.6,0.3,0.1])\n",
    "\n",
    "\n",
    "def Thompson_Sampling(n_arms, n_trials, alpha, beta):\n",
    "  theta = np.zeros(n_arms, dtype=float)\n",
    "  reward = np.zeros(n_arms, dtype=int)\n",
    "  for _ in range(n_trials):\n",
    "\n",
    "    for i in range(n_arms):\n",
    "    ## update\n",
    "      theta[i] = np.random.beta(alpha[i], beta[i])\n",
    "    \n",
    "    ## select\n",
    "    max_arm = np.argmax(theta)\n",
    "    cor_reward = np.random.binomial(1, theta[max_arm])\n",
    "    alpha[max_arm] += cor_reward\n",
    "    beta[max_arm] +=  (1 - cor_reward)\n",
    "  \n",
    "    reward[max_arm] += cor_reward\n",
    "\n",
    "  for i in range(n_arms):\n",
    "    ## final update\n",
    "    theta[i] = np.random.beta(alpha[i], beta[i])  \n",
    "  return np.sum(reward), theta\n",
    "\n",
    "\n",
    "def TS_simulation_mean(n_arms, n_trials, alpha_set, beta_set, set_index, n_experiment):\n",
    "  simu_reward_set = []\n",
    "  esti_theta_set = []\n",
    "\n",
    "  for i in range(n_experiment):\n",
    "    simu_reward, esti_theta = Thompson_Sampling(n_arms, n_trials, alpha_set[set_index], beta_set[set_index])\n",
    "    simu_reward_set.append(simu_reward)\n",
    "    esti_theta_set.append(esti_theta)\n",
    "\n",
    "  return np.mean(np.array(simu_reward_set)), esti_theta_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_trials = 5000\n",
    "arms = 3\n",
    "n_para_set = 3\n",
    "\n",
    "n_set = 2\n",
    "alpha_set = np.array([np.array([6, 3, 1]), np.array([60, 30, 10]), np.array([600, 300, 100])])\n",
    "beta_set = np.array([np.array([4, 7, 9]), np.array([40, 70, 90]),np.array([400, 700, 900])])\n",
    "\n",
    "n_experi = 200\n",
    "\n",
    "for i in range(n_para_set):\n",
    "  simu_reward, _ = TS_simulation_mean(arms, N_trials, alpha_set, beta_set, i, n_experi)\n",
    "  print(simu_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3269\n",
    "3251\n",
    "3018\n",
    "\n",
    "3714\n",
    "2437\n",
    "2930\n",
    "\n",
    "3427\n",
    "2913\n",
    "2972\n",
    "\n",
    "3677\n",
    "2971\n",
    "2982\n",
    "\n",
    "3016\n",
    "3244\n",
    "2995\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"Pulls per arm: {pulls}\")\n",
    "print(f\"Total rewards per arm: {rewards}\")\n",
    "print(f\"Cumulative regret: {regret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "class ThetaSequence:\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "def generate_theta_sequence_set(n_elem, n_sets):\n",
    "    theta_seq_set = [ThetaSequence(np.random.uniform(0, 1, size=n_elem)) for _ in range(n_sets)]\n",
    "    return theta_seq_set\n",
    "\n",
    "def simulate_epsilon_greedy(theta, epsilon, num_pulls_per_experiment):\n",
    "    num_arms = len(theta)\n",
    "    total_rewards = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    \n",
    "    for _ in range(num_pulls_per_experiment):\n",
    "        arm = np.random.randint(num_arms) if np.random.rand() < epsilon else np.argmax(theta)\n",
    "        counts[arm] += 1\n",
    "        reward = np.random.binomial(1, theta[arm])\n",
    "        total_rewards[arm] += reward\n",
    "\n",
    "    return total_rewards.sum()\n",
    "\n",
    "# 设置参数\n",
    "n_elem = 3\n",
    "n_sets = 5000\n",
    "epsilon = 0.1\n",
    "num_pulls_per_experiment = 5000\n",
    "\n",
    "# 生成ThetaSequence集合\n",
    "theta_seq_set = generate_theta_sequence_set(n_elem, n_sets)\n",
    "\n",
    "# 计算每个序列的平均值和标准差，并模拟epsilon-greedy算法\n",
    "theta_means = []\n",
    "theta_stds = []\n",
    "theta_rewards = []\n",
    "\n",
    "for theta_seq in theta_seq_set:\n",
    "    theta = theta_seq.seq\n",
    "    theta_means.append(np.mean(theta))\n",
    "    theta_stds.append(np.std(theta))\n",
    "    reward = simulate_epsilon_greedy(theta, epsilon, num_pulls_per_experiment)\n",
    "    theta_rewards.append(reward)\n",
    "\n",
    "# 绘制三维图像\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(theta_means, theta_stds, theta_rewards, c=theta_rewards, cmap='viridis', marker='o')\n",
    "\n",
    "ax.set_title(f'Theta Sequences (ε={epsilon})')\n",
    "ax.set_xlabel('Theta Mean')\n",
    "ax.set_ylabel('Theta Std Dev')\n",
    "ax.set_zlabel('Total Rewards')\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Total Rewards')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "class ThetaSequence:\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "def generate_theta_sequence_set(n_elem, n_sets):\n",
    "    theta_seq_set = [ThetaSequence(np.random.uniform(0, 1, size=n_elem)) for _ in range(n_sets)]\n",
    "    return theta_seq_set\n",
    "\n",
    "def simulate_epsilon_greedy(theta, epsilon, num_pulls_per_experiment):\n",
    "    num_arms = len(theta)\n",
    "    total_rewards = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    \n",
    "    for _ in range(num_pulls_per_experiment):\n",
    "        arm = np.random.randint(num_arms) if np.random.rand() < epsilon else np.argmax(theta)\n",
    "        counts[arm] += 1\n",
    "        reward = np.random.binomial(1, theta[arm])\n",
    "        total_rewards[arm] += reward\n",
    "\n",
    "    return total_rewards.sum()\n",
    "\n",
    "# 设置参数\n",
    "n_elem = 3\n",
    "n_sets = 50\n",
    "epsilon = 0.1\n",
    "num_pulls_per_experiment = 5000\n",
    "\n",
    "# 生成ThetaSequence集合\n",
    "theta_seq_set = generate_theta_sequence_set(n_elem, n_sets)\n",
    "\n",
    "# 计算每个序列的平均值和标准差，并模拟epsilon-greedy算法\n",
    "theta_means = []\n",
    "theta_stds = []\n",
    "theta_rewards = []\n",
    "\n",
    "for theta_seq in theta_seq_set:\n",
    "    theta = theta_seq.seq\n",
    "    theta_means.append(np.mean(theta))\n",
    "    theta_stds.append(np.std(theta))\n",
    "    reward = simulate_epsilon_greedy(theta, epsilon, num_pulls_per_experiment)\n",
    "    theta_rewards.append(reward)\n",
    "\n",
    "# 绘制三维图像，并添加旋转功能\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(theta_means, theta_stds, theta_rewards, c=theta_rewards, cmap='viridis', marker='o')\n",
    "\n",
    "ax.set_title(f'Theta Sequences (ε={epsilon})')\n",
    "ax.set_xlabel('Theta Mean')\n",
    "ax.set_ylabel('Theta Std Dev')\n",
    "ax.set_zlabel('Total Rewards')\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Total Rewards')\n",
    "\n",
    "# 更新函数，用于旋转视角\n",
    "def update(frame):\n",
    "    ax.view_init(elev=30., azim=frame)\n",
    "\n",
    "# 创建动画\n",
    "ani = FuncAnimation(fig, update, frames=np.arange(0, 360, 1), interval=100)\n",
    "\n",
    "# 显示动画\n",
    "plt.show()\n",
    "\n",
    "# 保存动画（如果需要）\n",
    "# ani.save('3d_scatter_rotation.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specific_theta_geneta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ThetaSequence:\n",
    "    def __init__(self, seq: np.ndarray, n_elements: int):\n",
    "        self.seq = seq\n",
    "        self.n_elem = n_elements\n",
    "        self.mean = np.mean(seq)\n",
    "        self.var = np.var(seq)\n",
    "        self.max_theta = np.max(seq)\n",
    "\n",
    "## randomly generate\n",
    "def generate_theta_sequence_set(n_elem, n_sets):\n",
    "    theta_seq_set = []\n",
    "    for _ in range(n_sets):\n",
    "        theta_seq = ThetaSequence(np.random.uniform(0, 1, size=n_elem), n_elem)\n",
    "        theta_seq_set.append(theta_seq)\n",
    "    return np.array(theta_seq_set)\n",
    "\n",
    "## 用theta组的数值特征，判断不同参数对不同特征的适应性\n",
    "## 每个theta 跑200次，每次5000下摇杆\n",
    "\n",
    "## 数值大小随意，数值特征重要\n",
    "# 均值0.05为跨度，标准差1/20为跨度\n",
    "\n",
    "## 使用固定的均值、标准差，生成n和数据（theta_seq）\n",
    "def specific_theta_generate(mean, std_dev, n):\n",
    "  # np.random.seed(0)  # 设置随机种子以确保结果可复现\n",
    "  flag = False\n",
    "  while not flag:\n",
    "    flag = True\n",
    "    theta_array = np.random.normal(loc=mean, scale=std_dev, size=n)\n",
    "    # for i in theta_array:\n",
    "    #   if i <= 0 or i >=1:\n",
    "    #     flag = False\n",
    "    #     break\n",
    "  return ThetaSequence(theta_array,n)\n",
    "\n",
    "## 静均值，动标准差\n",
    "def theta_generate_static_mean_rolling_std_dev(size, mean, start_std_dev, end_std_dev, span):\n",
    "  theta_set = []\n",
    "  for rolling_std_dev in np.arange(start_std_dev, end_std_dev, span):\n",
    "     theta_set.append(specific_theta_generate(mean, rolling_std_dev, size))\n",
    "  return np.array(theta_set)\n",
    "\n",
    "## 静标准差，动均值\n",
    "def theta_generate_static_std_dev_rolling_mean(size, std_dev, start_mean, end_mean, span):\n",
    "  theta_set = []\n",
    "  for rolling_mean in np.arange(start_mean, end_mean, span):\n",
    "     theta_set.append(specific_theta_generate(rolling_mean, std_dev, size))\n",
    "  return np.array(theta_set)\n",
    "\n",
    "\n",
    "# 已知的均值和方差\n",
    "known_mean = 0.5\n",
    "known_var = 1/6\n",
    "\n",
    "\n",
    "print(theta_generate_static_mean_rolling_std_dev(3,0.5,0.1,0.9,0.1)[2].mean)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
